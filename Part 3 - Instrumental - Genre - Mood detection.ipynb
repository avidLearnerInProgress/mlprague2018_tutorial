{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instrumental, Genre and Mood Detection in Music with Deep Learning\n",
    "\n",
    "This tutorial shows how different Convolutional Neural Network architectures are used for:\n",
    "* Instrumental vs. Vocal Detection:  detecting whether a piece of music is instrumental or contains vocals\n",
    "* Genre Classification\n",
    "* Mood Recognition\n",
    "\n",
    "The data set used is the [MagnaTagATune Dataset](http://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset), but a smaller subset of it, with only 1 sample excerpt of each of the original audio files.\n",
    "\n",
    "It consists of 5405 files, each 30 seconds long. \n",
    "\n",
    "The annotations for this dataset contain a multitude of tags, including some that hint at whether the file is instrumental or vocal. (see [Create 2 classes from a list of tags](#Create-2-classes-from-a-list-of-tags) below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "* Python 3.5\n",
    "* Keras >= 2.1.1\n",
    "* Tensorflow\n",
    "* scikit-learn >= 0.18\n",
    "* Pandas\n",
    "* Librosa\n",
    "* MatplotLib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "This tutorial contains:\n",
    "* Loading and Preprocessing of Audio files\n",
    "* Loading class files from CSV and using Label Encoder\n",
    "* Audio Preprocessing: Generating log Mel spectrograms\n",
    "* Standardization of Data\n",
    "* Convolutional Neural Networks\n",
    "* Train/Test set split\n",
    "\n",
    "* Instrumental vs. Vocal Detection\n",
    "* Genre Classification\n",
    "* Mood Recognition\n",
    "\n",
    "You can execute the following code blocks by pressing SHIFT+Enter consecutively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data\n",
    "\n",
    "The (subsampled) data set can be downloaded from [here](https://owncloud.tuwien.ac.at/index.php/s/hivOGXKoUQtacbo).\n",
    "\n",
    "Please unzip it.\n",
    "\n",
    "Set the path to the unpacked folder in the next box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_PATH = '/home/tlidy/data/MagnaTagATune'\n",
    "AUDIO_PATH = os.path.join(DATA_PATH, 'audio')\n",
    "META_PATH = os.path.join(DATA_PATH, 'metadata')\n",
    "\n",
    "#NPZ_FILE = '/home/tlidy/data/mel_spectrogram_segments_96x1366.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET GPUs to use:\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #\"0,1,2,3\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# General Imports\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import datetime\n",
    "import glob\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd # Pandas for reading CSV files and easier Data handling in preparation\n",
    "\n",
    "# Deep Learning\n",
    "\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Convolution2D, MaxPooling2D, Dense, Dropout, Activation, Flatten, merge\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import ELU\n",
    "\n",
    "# Machine Learning preprocessing and evaluation\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local imports for audio reading and processing\n",
    "import audio_spectrogram as rp\n",
    "from audiofile_read import audiofile_read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Metadata\n",
    "\n",
    "The tab-separated file contains pairs of filename TAB class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clip_id</th>\n",
       "      <th>no voice</th>\n",
       "      <th>singer</th>\n",
       "      <th>duet</th>\n",
       "      <th>plucking</th>\n",
       "      <th>hard rock</th>\n",
       "      <th>world</th>\n",
       "      <th>bongos</th>\n",
       "      <th>harpsichord</th>\n",
       "      <th>female singing</th>\n",
       "      <th>...</th>\n",
       "      <th>female singer</th>\n",
       "      <th>rap</th>\n",
       "      <th>metal</th>\n",
       "      <th>hip hop</th>\n",
       "      <th>quick</th>\n",
       "      <th>water</th>\n",
       "      <th>baroque</th>\n",
       "      <th>women</th>\n",
       "      <th>fiddle</th>\n",
       "      <th>english</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mp3_path</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>d/ambient_teknology-the_all_seeing_eye_project-01-cyclops-262-291.mp3</th>\n",
       "      <td>1119</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d/ambient_teknology-the_all_seeing_eye_project-02-all_seeing_eye-175-204.mp3</th>\n",
       "      <td>6021</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d/ambient_teknology-the_all_seeing_eye_project-03-black-175-204.mp3</th>\n",
       "      <td>11847</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d/ambient_teknology-the_all_seeing_eye_project-04-confusion_says-88-117.mp3</th>\n",
       "      <td>17119</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d/ambient_teknology-the_all_seeing_eye_project-05-the_beholder-291-320.mp3</th>\n",
       "      <td>25118</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d/ambient_teknology-the_all_seeing_eye_project-06-cyead-378-407.mp3</th>\n",
       "      <td>26533</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d/ambient_teknology-the_all_seeing_eye_project-07-telekonology-117-146.mp3</th>\n",
       "      <td>33637</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0/american_bach_soloists-j_s__bach__cantatas_volume_v-01-gleichwie_der_regen_und_schnee_vom_himmel_fallt_bwv_18_i_sinfonia-117-146.mp3</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0/american_bach_soloists-j_s__bach__cantatas_volume_v-02-gleichwie_der_regen_und_schnee_vom_himmel_fallt_bwv_18_ii_recitative__gleichwie_der_regen_und_schnee-30-59.mp3</th>\n",
       "      <td>5864</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0/american_bach_soloists-j_s__bach__cantatas_volume_v-03-gleichwie_der_regen_und_schnee_vom_himmel_fallt_bwv_18_iii_recitative_and_litany__mein_gott_hier_wird_mein_herze_sein-88-117.mp3</th>\n",
       "      <td>11264</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 189 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    clip_id  no voice  singer  \\\n",
       "mp3_path                                                                        \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...     1119         0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...     6021         0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...    11847         0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...    17119         0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...    25118         0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...    26533         0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...    33637         0       0   \n",
       "0/american_bach_soloists-j_s__bach__cantatas_vo...       29         0       0   \n",
       "0/american_bach_soloists-j_s__bach__cantatas_vo...     5864         0       0   \n",
       "0/american_bach_soloists-j_s__bach__cantatas_vo...    11264         0       0   \n",
       "\n",
       "                                                    duet  plucking  hard rock  \\\n",
       "mp3_path                                                                        \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...     0         0          0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...     0         0          0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...     0         0          0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...     0         0          0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...     0         0          0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...     0         0          0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...     0         0          0   \n",
       "0/american_bach_soloists-j_s__bach__cantatas_vo...     0         0          0   \n",
       "0/american_bach_soloists-j_s__bach__cantatas_vo...     0         0          0   \n",
       "0/american_bach_soloists-j_s__bach__cantatas_vo...     0         0          0   \n",
       "\n",
       "                                                    world  bongos  \\\n",
       "mp3_path                                                            \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0       0   \n",
       "0/american_bach_soloists-j_s__bach__cantatas_vo...      0       0   \n",
       "0/american_bach_soloists-j_s__bach__cantatas_vo...      0       0   \n",
       "0/american_bach_soloists-j_s__bach__cantatas_vo...      0       0   \n",
       "\n",
       "                                                    harpsichord  \\\n",
       "mp3_path                                                          \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...            0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...            0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...            0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...            0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...            0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...            0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...            0   \n",
       "0/american_bach_soloists-j_s__bach__cantatas_vo...            0   \n",
       "0/american_bach_soloists-j_s__bach__cantatas_vo...            0   \n",
       "0/american_bach_soloists-j_s__bach__cantatas_vo...            0   \n",
       "\n",
       "                                                    female singing   ...     \\\n",
       "mp3_path                                                             ...      \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...               0   ...      \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...               0   ...      \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...               0   ...      \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...               0   ...      \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...               0   ...      \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...               0   ...      \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...               0   ...      \n",
       "0/american_bach_soloists-j_s__bach__cantatas_vo...               0   ...      \n",
       "0/american_bach_soloists-j_s__bach__cantatas_vo...               0   ...      \n",
       "0/american_bach_soloists-j_s__bach__cantatas_vo...               0   ...      \n",
       "\n",
       "                                                    female singer  rap  metal  \\\n",
       "mp3_path                                                                        \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...              0    0      0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...              0    0      0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...              0    0      0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...              0    0      0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...              0    0      0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...              0    0      0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...              0    0      0   \n",
       "0/american_bach_soloists-j_s__bach__cantatas_vo...              0    0      0   \n",
       "0/american_bach_soloists-j_s__bach__cantatas_vo...              0    0      0   \n",
       "0/american_bach_soloists-j_s__bach__cantatas_vo...              0    0      0   \n",
       "\n",
       "                                                    hip hop  quick  water  \\\n",
       "mp3_path                                                                    \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...        0      0      0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...        0      0      0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...        0      0      0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...        0      0      0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...        0      0      0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...        0      0      0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...        0      0      0   \n",
       "0/american_bach_soloists-j_s__bach__cantatas_vo...        0      0      0   \n",
       "0/american_bach_soloists-j_s__bach__cantatas_vo...        0      0      0   \n",
       "0/american_bach_soloists-j_s__bach__cantatas_vo...        0      0      0   \n",
       "\n",
       "                                                    baroque  women  fiddle  \\\n",
       "mp3_path                                                                     \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...        0      0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...        0      0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...        0      0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...        0      0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...        0      0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...        0      0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...        0      0       0   \n",
       "0/american_bach_soloists-j_s__bach__cantatas_vo...        0      0       0   \n",
       "0/american_bach_soloists-j_s__bach__cantatas_vo...        0      0       0   \n",
       "0/american_bach_soloists-j_s__bach__cantatas_vo...        0      0       0   \n",
       "\n",
       "                                                    english  \n",
       "mp3_path                                                     \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...        0  \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...        0  \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...        0  \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...        0  \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...        0  \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...        0  \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...        0  \n",
       "0/american_bach_soloists-j_s__bach__cantatas_vo...        0  \n",
       "0/american_bach_soloists-j_s__bach__cantatas_vo...        0  \n",
       "0/american_bach_soloists-j_s__bach__cantatas_vo...        0  \n",
       "\n",
       "[10 rows x 189 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file = os.path.join(META_PATH,'annotations_final_subsample.csv')\n",
    "\n",
    "# we select the last column (-1) as the index column (= filename)\n",
    "metadata = pd.read_csv(csv_file, index_col=0, sep='\\t')\n",
    "metadata.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no voice</th>\n",
       "      <th>singer</th>\n",
       "      <th>duet</th>\n",
       "      <th>plucking</th>\n",
       "      <th>hard rock</th>\n",
       "      <th>world</th>\n",
       "      <th>bongos</th>\n",
       "      <th>harpsichord</th>\n",
       "      <th>female singing</th>\n",
       "      <th>clasical</th>\n",
       "      <th>...</th>\n",
       "      <th>female singer</th>\n",
       "      <th>rap</th>\n",
       "      <th>metal</th>\n",
       "      <th>hip hop</th>\n",
       "      <th>quick</th>\n",
       "      <th>water</th>\n",
       "      <th>baroque</th>\n",
       "      <th>women</th>\n",
       "      <th>fiddle</th>\n",
       "      <th>english</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mp3_path</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>d/ambient_teknology-the_all_seeing_eye_project-01-cyclops-262-291.mp3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d/ambient_teknology-the_all_seeing_eye_project-02-all_seeing_eye-175-204.mp3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d/ambient_teknology-the_all_seeing_eye_project-03-black-175-204.mp3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d/ambient_teknology-the_all_seeing_eye_project-04-confusion_says-88-117.mp3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d/ambient_teknology-the_all_seeing_eye_project-05-the_beholder-291-320.mp3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 188 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    no voice  singer  duet  \\\n",
       "mp3_path                                                                     \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...         0       0     0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...         0       0     0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...         0       0     0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...         0       0     0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...         0       0     0   \n",
       "\n",
       "                                                    plucking  hard rock  \\\n",
       "mp3_path                                                                  \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...         0          0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...         0          0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...         0          0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...         0          0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...         0          0   \n",
       "\n",
       "                                                    world  bongos  \\\n",
       "mp3_path                                                            \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0       0   \n",
       "\n",
       "                                                    harpsichord  \\\n",
       "mp3_path                                                          \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...            0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...            0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...            0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...            0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...            0   \n",
       "\n",
       "                                                    female singing  clasical  \\\n",
       "mp3_path                                                                       \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...               0         0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...               0         0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...               0         0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...               0         0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...               0         0   \n",
       "\n",
       "                                                     ...     female singer  \\\n",
       "mp3_path                                             ...                     \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...   ...                 0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...   ...                 0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...   ...                 0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...   ...                 0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...   ...                 0   \n",
       "\n",
       "                                                    rap  metal  hip hop  \\\n",
       "mp3_path                                                                  \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...    0      0        0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...    0      0        0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...    0      0        0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...    0      0        0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...    0      0        0   \n",
       "\n",
       "                                                    quick  water  baroque  \\\n",
       "mp3_path                                                                    \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0      0        0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0      0        0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0      0        0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0      0        0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0      0        0   \n",
       "\n",
       "                                                    women  fiddle  english  \n",
       "mp3_path                                                                    \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0       0        0  \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0       0        0  \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0       0        0  \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0       0        0  \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0       0        0  \n",
       "\n",
       "[5 rows x 188 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the unneeded column \"clip_id\"\n",
    "cols = \"clip_id\"\n",
    "metadata.drop(cols,axis=1,inplace=True)\n",
    "\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Instrumental vs. Vocal Detection\n",
    "\n",
    "this is a binary classification task (output decision is between 0 and 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 2 classes from a list of tags\n",
    "\n",
    "There are plenty of \"tags\" in this data set which hint at wether a track is \"vocal\" or \"instrumental\". We group these tags and finally come up with 1 boolean column saying whether a track is \"vocal\" or \"instrumental\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_vocal = ['singer', 'female singing', 'female opera', 'male vocal', 'vocals', 'men', 'female', 'female voice', 'voice', 'male voice', 'girl', 'chanting', 'talking', 'choral', 'male singer', 'man singing', 'male opera', 'chant', 'man', 'female vocal', 'male vocals', 'vocal', 'woman', 'woman singing', 'singing', 'female vocals', 'voices', 'choir', 'female singer', 'women', 'choir', 'women']\n",
    "\n",
    "tags_instrumental = ['instrumental', 'no voice', 'no voices', 'no vocals', 'no vocal', 'no singing', 'no singer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>singer</th>\n",
       "      <th>female singing</th>\n",
       "      <th>female opera</th>\n",
       "      <th>male vocal</th>\n",
       "      <th>vocals</th>\n",
       "      <th>men</th>\n",
       "      <th>female</th>\n",
       "      <th>female voice</th>\n",
       "      <th>voice</th>\n",
       "      <th>male voice</th>\n",
       "      <th>...</th>\n",
       "      <th>woman</th>\n",
       "      <th>woman singing</th>\n",
       "      <th>singing</th>\n",
       "      <th>female vocals</th>\n",
       "      <th>voices</th>\n",
       "      <th>choir</th>\n",
       "      <th>female singer</th>\n",
       "      <th>women</th>\n",
       "      <th>choir</th>\n",
       "      <th>women</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mp3_path</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>d/ambient_teknology-the_all_seeing_eye_project-01-cyclops-262-291.mp3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d/ambient_teknology-the_all_seeing_eye_project-02-all_seeing_eye-175-204.mp3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d/ambient_teknology-the_all_seeing_eye_project-03-black-175-204.mp3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d/ambient_teknology-the_all_seeing_eye_project-04-confusion_says-88-117.mp3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d/ambient_teknology-the_all_seeing_eye_project-05-the_beholder-291-320.mp3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    singer  female singing  \\\n",
       "mp3_path                                                                     \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...       0               0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...       0               0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...       0               0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...       0               0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...       0               0   \n",
       "\n",
       "                                                    female opera  male vocal  \\\n",
       "mp3_path                                                                       \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...             0           0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...             0           0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...             0           0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...             0           0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...             0           0   \n",
       "\n",
       "                                                    vocals  men  female  \\\n",
       "mp3_path                                                                  \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...       0    0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...       0    0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...       0    0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...       0    0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...       0    0       0   \n",
       "\n",
       "                                                    female voice  voice  \\\n",
       "mp3_path                                                                  \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...             0      0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...             0      0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...             0      0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...             0      0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...             0      0   \n",
       "\n",
       "                                                    male voice  ...    woman  \\\n",
       "mp3_path                                                        ...            \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...           0  ...        0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...           0  ...        0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...           0  ...        0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...           0  ...        0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...           0  ...        0   \n",
       "\n",
       "                                                    woman singing  singing  \\\n",
       "mp3_path                                                                     \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...              0        0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...              0        0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...              0        0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...              0        0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...              0        0   \n",
       "\n",
       "                                                    female vocals  voices  \\\n",
       "mp3_path                                                                    \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...              0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...              0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...              0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...              0       0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...              0       0   \n",
       "\n",
       "                                                    choir  female singer  \\\n",
       "mp3_path                                                                   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0              0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0              0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0              0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0              0   \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0              0   \n",
       "\n",
       "                                                    women  choir  women  \n",
       "mp3_path                                                                 \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0      0      0  \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0      0      0  \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0      0      0  \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0      0      0  \n",
       "d/ambient_teknology-the_all_seeing_eye_project-...      0      0      0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata[tags_vocal].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mp3_path\n",
       "d/ambient_teknology-the_all_seeing_eye_project-01-cyclops-262-291.mp3           False\n",
       "d/ambient_teknology-the_all_seeing_eye_project-02-all_seeing_eye-175-204.mp3    False\n",
       "d/ambient_teknology-the_all_seeing_eye_project-03-black-175-204.mp3             False\n",
       "d/ambient_teknology-the_all_seeing_eye_project-04-confusion_says-88-117.mp3     False\n",
       "d/ambient_teknology-the_all_seeing_eye_project-05-the_beholder-291-320.mp3      False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set vocal to True of any of the tags_vocal are 1\n",
    "gt_vocal = metadata[tags_vocal].any(axis=1)\n",
    "gt_vocal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mp3_path\n",
       "d/ambient_teknology-the_all_seeing_eye_project-01-cyclops-262-291.mp3           False\n",
       "d/ambient_teknology-the_all_seeing_eye_project-02-all_seeing_eye-175-204.mp3    False\n",
       "d/ambient_teknology-the_all_seeing_eye_project-03-black-175-204.mp3             False\n",
       "d/ambient_teknology-the_all_seeing_eye_project-04-confusion_says-88-117.mp3     False\n",
       "d/ambient_teknology-the_all_seeing_eye_project-05-the_beholder-291-320.mp3       True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set instrumental to True of any of the tags_instrumental are 1\n",
    "gt_instrumental = metadata[tags_instrumental].any(axis=1)\n",
    "gt_instrumental.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>We can only use the tag if EITHER instrumental OR vocal is True.</b><br>\n",
    "If both of them are True or both of them are False, we cannot trust the groundtruth data. Ergo we have to remove these and retain only the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mp3_path\n",
       "d/ambient_teknology-the_all_seeing_eye_project-01-cyclops-262-291.mp3           False\n",
       "d/ambient_teknology-the_all_seeing_eye_project-02-all_seeing_eye-175-204.mp3    False\n",
       "d/ambient_teknology-the_all_seeing_eye_project-03-black-175-204.mp3             False\n",
       "d/ambient_teknology-the_all_seeing_eye_project-04-confusion_says-88-117.mp3     False\n",
       "d/ambient_teknology-the_all_seeing_eye_project-05-the_beholder-291-320.mp3       True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retain = np.logical_xor(gt_vocal,gt_instrumental)\n",
    "retain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From originally 3023 input examples, we can only retain 959 trusted ones in our groundtruth\n"
     ]
    }
   ],
   "source": [
    "n_orig = len(gt_vocal)\n",
    "\n",
    "n_retain = sum(retain)\n",
    "\n",
    "print(\"From originally\", n_orig, \"input examples, we can only retain\",n_retain, \"trusted ones in our groundtruth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end we cut from gt_vocal only the exampls to retain. If they are True they are vocal, if they are False, they are instrumental:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mp3_path\n",
       "d/ambient_teknology-the_all_seeing_eye_project-05-the_beholder-291-320.mp3                                                                                                                   False\n",
       "0/american_bach_soloists-j_s__bach__cantatas_volume_v-03-gleichwie_der_regen_und_schnee_vom_himmel_fallt_bwv_18_iii_recitative_and_litany__mein_gott_hier_wird_mein_herze_sein-88-117.mp3     True\n",
       "0/american_bach_soloists-j_s__bach__cantatas_volume_v-07-weinen_klagen_sorgen_zagen_bwv_12_ii_chorus__weinen_klagen_sorgen_zagen-262-291.mp3                                                  True\n",
       "0/american_bach_soloists-j_s__bach__cantatas_volume_v-08-weinen_klagen_sorgen_zagen_bwv_12_iii_recitative__wir_mussen_durch_viel_truebsal-0-29.mp3                                            True\n",
       "0/american_bach_soloists-j_s__bach__cantatas_volume_v-09-weinen_klagen_sorgen_zagen_bwv_12_iv_aria__kreuz_und_krone_sind_verbunden-59-88.mp3                                                  True\n",
       "0/american_bach_soloists-j_s__bach__cantatas_volume_v-11-weinen_klagen_sorgen_zagen_bwv_12_vi_aria__sei_getreu-88-117.mp3                                                                     True\n",
       "0/american_bach_soloists-j_s__bach__cantatas_volume_v-13-nun_komm_der_heiden_heiland_bwv_61_i_ouverture__nun_komm_der_heiden_heiland-59-88.mp3                                                True\n",
       "0/american_bach_soloists-j_s__bach__cantatas_volume_v-14-nun_komm_der_heiden_heiland_bwv_61_ii_recitative__der_heiland_ist_gekommen-30-59.mp3                                                 True\n",
       "0/american_bach_soloists-j_s__bach__cantatas_volume_v-15-nun_komm_der_heiden_heiland_bwv_61_iii_aria__komm_jesu_komm_zu_deiner_kirche-117-146.mp3                                             True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_final = gt_vocal[retain]\n",
    "gt_final.head(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "671 vocal tracks\n"
     ]
    }
   ],
   "source": [
    "print(str(sum(gt_final)) + \" vocal tracks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288 instrumental tracks\n"
     ]
    }
   ],
   "source": [
    "print(str(sum(np.logical_not(gt_final))) + \" instrumental tracks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Create two lists: one with filenames and one with associated classes</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index in list of strings\n",
    "filelist = gt_final.index.tolist()\n",
    "# convert boolean to int and store in other list\n",
    "classes = (gt_final * 1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d/ambient_teknology-the_all_seeing_eye_project-05-the_beholder-291-320.mp3',\n",
       " '0/american_bach_soloists-j_s__bach__cantatas_volume_v-03-gleichwie_der_regen_und_schnee_vom_himmel_fallt_bwv_18_iii_recitative_and_litany__mein_gott_hier_wird_mein_herze_sein-88-117.mp3',\n",
       " '0/american_bach_soloists-j_s__bach__cantatas_volume_v-07-weinen_klagen_sorgen_zagen_bwv_12_ii_chorus__weinen_klagen_sorgen_zagen-262-291.mp3',\n",
       " '0/american_bach_soloists-j_s__bach__cantatas_volume_v-08-weinen_klagen_sorgen_zagen_bwv_12_iii_recitative__wir_mussen_durch_viel_truebsal-0-29.mp3',\n",
       " '0/american_bach_soloists-j_s__bach__cantatas_volume_v-09-weinen_klagen_sorgen_zagen_bwv_12_iv_aria__kreuz_und_krone_sind_verbunden-59-88.mp3']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filelist[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to Numpy array as needed by Keras\n",
    "classes = np.array(classes)\n",
    "classes[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Audio Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to analyze audio files and get small or large spectrogram excerpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spectrograms(filelist, n_mel_bands=40, frames=80, return_example=False):\n",
    "\n",
    "    list_spectrograms = [] # spectrograms are put into a list first\n",
    "\n",
    "    # some FFT parameters\n",
    "    fft_window_size=512\n",
    "    fft_overlap = 0.5\n",
    "    hop_size = int(fft_window_size*(1-fft_overlap))\n",
    "    segment_size = fft_window_size + (frames-1) * hop_size # segment size for desired # frames\n",
    "\n",
    "    print(\"Reading and processing\", len(filelist), \"audio files\")\n",
    "    \n",
    "    for filename in filelist:\n",
    "        print(\".\", end='')\n",
    "        filepath = os.path.join(AUDIO_PATH, filename)\n",
    "        samplerate, samplewidth, wavedata = audiofile_read(filepath,verbose=False)\n",
    "        sample_length = wavedata.shape[0]\n",
    "\n",
    "        # make Mono (in case of multiple channels / stereo)\n",
    "        if wavedata.ndim > 1:\n",
    "            wavedata = np.mean(wavedata, 1)\n",
    "\n",
    "        # take only a segment; choose start position:\n",
    "        #pos = 0 # beginning\n",
    "        pos = int(wavedata.shape[0]/2 - segment_size/2) # center minus half segment size\n",
    "        wav_segment = wavedata[pos:pos+segment_size]\n",
    "\n",
    "        # 1) FFT spectrogram \n",
    "        spectrogram = rp.calc_spectrogram(wav_segment,fft_window_size,fft_overlap)\n",
    "\n",
    "        # 2) Transform to perceptual Mel scale (uses librosa.filters.mel)\n",
    "        spectrogram = rp.transform2mel(spectrogram,samplerate,fft_window_size,n_mel_bands)\n",
    "\n",
    "        # 3) Log 10 transform\n",
    "        spectrogram = np.log10(spectrogram)\n",
    "\n",
    "        list_spectrograms.append(spectrogram)\n",
    "\n",
    "    print(\"\\nConverting to big data array...\")\n",
    "    # a list of many spectrograms is made into 1 big array with 3 dimensions\n",
    "    # + convert the input data to the right data type used by Keras Deep Learning (GPU)\n",
    "    data = np.array(list_spectrograms, dtype=K.floatx())\n",
    "\n",
    "    # replace Inf values:\n",
    "    # as in our preprocessing some files generated an Inf value in the log10 computation, we replace those by 0:\n",
    "\n",
    "    data[np.isinf(data)] = 0\n",
    "\n",
    "    print(\"done.\")\n",
    "    \n",
    "    # just for illustration purposes, return the last wav file and its spectrogram and audio data\n",
    "    if return_example:\n",
    "        return data, wav_segment, spectrogram, segment_size, samplerate, samplewidth\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define desired output parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small spectrograms\n",
    "#n_mel_bands = 40   # y axis\n",
    "#frames = 80        # x axis\n",
    "\n",
    "# large  spectrograms\n",
    "n_mel_bands = 96   # y axis\n",
    "frames = 683        # x axis\n",
    "\n",
    "# extra large  spectrograms\n",
    "#n_mel_bands = 96   # y axis\n",
    "#frames = 1366        # x axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we saved the audio spectrograms before, we try to load them\n",
    "load_features = True\n",
    "\n",
    "# if not, we store audio features for faster reload the next time\n",
    "save_features = True\n",
    "\n",
    "FEAT_FILE = os.path.join(DATA_PATH, \"spectrograms_instrumental.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded features successfully: 959 files, dimensions: (959, 96, 683)\n"
     ]
    }
   ],
   "source": [
    "if load_features:\n",
    "    if os.path.exists(FEAT_FILE):\n",
    "        with np.load(FEAT_FILE) as npz:\n",
    "            data = npz['data']\n",
    "            filelist = npz['filenames']\n",
    "            #classes = npz['classes']\n",
    "        print(\"Loaded features successfully: \" + str(len(filelist)), \"files, dimensions:\", data.shape)\n",
    "    else:\n",
    "        load_features = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_features:\n",
    "    #data = create_spectrograms(filelist, n_mel_bands, frames)\n",
    "\n",
    "    # get some extra data for illustration\n",
    "    data, wav_segment, spectrogram, segment_size, samplerate, samplewidth = create_spectrograms(filelist, n_mel_bands, frames, return_example=True)\n",
    "    \n",
    "    if save_features:\n",
    "        np.savez(FEAT_FILE, data=data, filenames=filelist, classes=classes)\n",
    "        print(\"Features stored to \" + FEAT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Waveform and Spectrogram (just for illustration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_features:\n",
    "    print(samplerate, samplewidth)\n",
    "    print(spectrogram.shape)\n",
    "    print(data.shape)\n",
    "    print(\"An audio segment is\", round(float(segment_size) / samplerate, 2), \"seconds long\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_features:\n",
    "    print(wav_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can skip this if you do not have matplotlib installed\n",
    "\n",
    "if not load_features:\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline \n",
    "\n",
    "    # show 1 sec wave segment\n",
    "    plt.plot(wav_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show spectrogram\n",
    "\n",
    "if not load_features:\n",
    "    fig = plt.imshow(spectrogram, origin='lower', aspect='auto')\n",
    "    fig.set_cmap('jet')\n",
    "    fig.axes.get_xaxis().set_visible(False)\n",
    "    fig.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "\n",
    "<b>Always standardize</b> the data before feeding it into the Neural Network!\n",
    "\n",
    "We use <b>Zero-mean Unit-variance standardization</b> (also known as Z-score normalization).\n",
    "Here, we use <b>attribute-wise standardization</b>, i.e. each pixel is standardized individually, as opposed to computing a single mean and single standard deviation of all values.\n",
    "\n",
    "('Flat' standardization would also be possible, but we have seen benefits of attribut-wise standardization in our experiments).\n",
    "\n",
    "We use the StandardScaler from the scikit-learn package for our purpose.\n",
    "As it works typically on vector data, we have to vectorize (i.e. reshape) our matrices first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(data):\n",
    "    # vectorize before standardization (cause scaler can't do it in that format)\n",
    "    N, ydim, xdim = data.shape\n",
    "    data = data.reshape(N, xdim*ydim)\n",
    "\n",
    "    # standardize\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    data = scaler.fit_transform(data)\n",
    "\n",
    "    # reshape to original shape\n",
    "    return data.reshape(N, ydim, xdim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = standardize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "A Convolutional Neural Network (ConvNet or CNN) is a type of (deep) Neural Network that is well-suited for 2D axes data, such as images or spectrograms, as it is optimized for learning from spatial proximity. Its core elements are 2D filter kernels which essentially learn the weights of the Neural Network, and downscaling functions such as Max Pooling.\n",
    "\n",
    "A CNN can have one or more Convolution layers, each of them having an arbitrary number of N filters (which define the depth of the CNN layer), following typically by a pooling step, which aggregates neighboring pixels together and thus reduces the image resolution by retaining only the maximum values of neighboring pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "\n",
    "### Adding the channel\n",
    "\n",
    "As CNNs were initially made for image data, we need to add a dimension for the color channel to the data. RGB images typically have a 3rd dimension with the color. \n",
    "\n",
    "<b>Spectrograms, however, are considered like greyscale images, as in the previous tutorial.\n",
    "Likewise we need to add an extra dimension for compatibility with the CNN implementation.</b>\n",
    "\n",
    "For greyscale images, we add the number 1 as the depth of the additional dimension of the input shape (for RGB color images, the number of channels is 3).\n",
    "\n",
    "<i>Note on Tensorflow vs. Theano:</i>\n",
    "\n",
    "In Theano, traditionally the color channel was the <b>first</b> dimension in the image shape. \n",
    "In Tensorflow, the color channel is the <b>last</b> dimension in the image shape. \n",
    "\n",
    "This can be configured in ~/.keras/keras.json: \"image_dim_ordering\": \"th\" or \"tf\" (for Theano or Tensorflow) *or* with \"image_data_format\" set to \"channels_first\" or \"channels_last\".\n",
    "\n",
    "Tensorflow is now the default image ordering for Kears (\"tf\" and/or \"channels_last\").\n",
    "To be on the safe side, we added the if statement below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'channels_last'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.image_data_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_channel(data, n_channels=1):\n",
    "    # n_channels: 1 for grey-scale, 3 for RGB, but usually already present in the data\n",
    "    \n",
    "    N, ydim, xdim = data.shape\n",
    "\n",
    "    if keras.backend.image_data_format() == 'channels_last':  # TENSORFLOW\n",
    "        # Tensorflow ordering (~/.keras/keras.json: \"image_dim_ordering\": \"tf\")\n",
    "        data = data.reshape(N, ydim, xdim, n_channels)\n",
    "    else: # THEANO\n",
    "        # Theano ordering (~/.keras/keras.json: \"image_dim_ordering\": \"th\")\n",
    "        data = data.reshape(N, n_channels, ydim, xdim)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(959, 96, 683, 1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = add_channel(data, n_channels=1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 683, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we store the new shape of the images in the 'input_shape' variable.\n",
    "# take all dimensions except the 0th one (which is the number of files)\n",
    "input_shape = data.shape[1:]  \n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Test Set Split\n",
    "\n",
    "We split the original full data set into two parts: Train Set (75%) and Test Set (25%).\n",
    "\n",
    "Note: \n",
    "For demo purposes we use only 1 split here. A better way to do it is to use **Cross-Validation**, doing the split multiple times, iterating training and testing over the splits and averaging the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_size = 0.25 # % portion of whole data set to keep for testing, i.e. 75% is used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN INDEX: [135 957 350 397 651 154 704 255 811 881 937 921 229 635 839 643   2 619\n",
      " 299 225 388 400 717 444 736 673 675 548 731 850 401 112 608  71 791  76\n",
      " 281 721 545 538 155 753 206  41 527 856 491 745 366 756 720 227 218 836\n",
      " 103 711  72 878  42 144 447 349 589 710 362 203 376 864 782 226 302 220\n",
      " 379 773 256 555 476 231  49  51 601 458 744 623 906 685 136 730 115 781\n",
      " 747 628 869 441 148 728 264 934  37 479 599 237 134 493 557 265 687  88\n",
      " 363 884 179 201 195 578 166 342 797 339 842 284 924 416 145 382  22  80\n",
      " 851 927  60 952 912 292 378 634 287 263 181 316 954  53 141 928 278 348\n",
      " 563 890 735 553  93 385 564 294 824 371 841 585 198 757 700 876   3 558\n",
      " 953 748 678 903 544 920 784  16 832 466 448 883 190 187 200 483 650 654\n",
      " 128 442  75 752 914 894 776 386 100  30 178   1 249 420 129 101 497 429\n",
      "  61 888 450 396 480 172 923 885 694 693 916 373 909 273 624 621 331  55\n",
      "  39 570 104 234 785 672  86 270 475  54  95 569 146  59 603 521 543 568\n",
      " 403 726  47 702 577  94  12 216 267 760 471 828 482 300 829 768 242 413\n",
      " 617 729 767 436  18 795 321 542 758 790 549 664  32 922 514 461 175 830\n",
      " 293 592 827 106 304 676 949 706 246  43 825 199 233 235 739 431 674 337\n",
      "  99 254 866 610 682 525 121 662 633 587 367 434 245 370 595 122 556 383\n",
      " 171 251 193 309 541 656 897   9 318 737 365 932 435 554 207  70 798 833\n",
      " 325   6 408 566 637 755 740 770 783 162 852 327 831 140 474 955 212 590\n",
      " 290 534 933  11   7 271 574  44 573 150 138 399 616 695 387 307 262 733\n",
      " 481  84 772 666 594 355  77 426 130 261  34 714 918 948 392 208 872 605\n",
      " 503  50 636 854 809 137 528  24 422 346 646 865 668 340  36 683  27 313\n",
      "  57 751 192 301 295  38 170 407 907 501 238 323 456 205 391 642 329 679\n",
      " 180 855 796  40 722 451  13 336 707 276 185 488 421 320 202  58 495 513\n",
      " 536 526 507 584 667 411 763 696 631 186 788  52 404 925 579 862 703 905\n",
      "  97 861 716 163 804 412 390 613 464 698  64 532 377 818 277 575  83 500\n",
      " 241 808 283 232 125 472 893 762 402 789 561 596 230 496 452 248 410  79\n",
      "  46 459 517 268 253 512 457 539 467 361 494 196  89 640  29  87 843 473\n",
      " 372  65 699  15  19 847 586 820 465 142 222 826 778 571 823 659 462 215\n",
      " 580 303  85 357 780  74 351 223 519 478 417 343 567 936 857 484 338  78\n",
      " 581 591 626 124 882 560 219 260 153 738 638 806 425 424 279   0  23 648\n",
      " 105 794 116 240 946 689  56  48 224 669 449 213 460 951 750 742 805 661\n",
      " 904 209 799 766 546 469 612 317 428 498 838 347 734 108 793 418 620  10\n",
      " 688 814 364 725 236 341 285 427 845 272 547 443 509 123 902 853 381 746\n",
      " 565  81 606 310 898 258 615 598 705 210 718 405 328 131 432 113  20 868\n",
      " 732 530 761 871 614 873 531 803 446 840 652 552 133  21 899 194 649 167\n",
      " 508 315 311 754 681 870 118 324 380 690  68 147  67 306  91 244 152 522\n",
      " 801 250 360 505 117  96 950 344  92 913 812 247  35 298 184 139 879 723\n",
      " 641 282 727 609 697  33 671 161 114 352 764 454 437 625 414 515 708 393\n",
      " 169 111 741 802 297 243 655 419 291 941 821 394 712 956 627 749 919]\n",
      "TEST INDEX: [800 158 816 468 197 680 529 463 516 958 354 947 769 438 665 333 330 917\n",
      " 891 415 588   8 151  45 597 204 239 813 374 938 880 499 433 489 844 691\n",
      " 834 440 647 867 107 322 817 692 901 319 931 191 848 375 485 368 334 520\n",
      " 774 518 940 353 819 858 406 143 157 622 860   5 792 645 359 266 510 132\n",
      " 572  17 211 653 535  98 176 455 849 775 945 164  63 670 188 686 228  62\n",
      " 896 398 286 660 877 630 308 779 120 477 332 926 743 280 326 787   4 430\n",
      " 506 859 944 930 658 214 409 356 314 895  25 629 550 551 423 771 713 701\n",
      " 274 892 611 943 663 709 395 504 453 109 910 942 177 684 576 156 369 777\n",
      "  90 846 445 822 810 149 639  14 523  73 604 759 540 389 252 492 889  28\n",
      " 335 312 677 863 345 533 908 384 600 874  66 168 110 607 939 524 875 935\n",
      " 837 275 439 470 502 583 288 183 217 486 182 119 305 644 911 487 886 189\n",
      " 126 358 724  69 296 715 618  82 165 582 657 807  31 257 929 559 786 562\n",
      " 160 815 490 900 289 269 173  26 719 915 632 593 835 537 259 511 127 765\n",
      " 887 221 602 174 159 102]\n"
     ]
    }
   ],
   "source": [
    "# Stratified Split retains the class balance in both sets\n",
    "\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=testset_size, random_state=0)\n",
    "splits = splitter.split(data, classes)\n",
    "\n",
    "for train_index, test_index in splits:\n",
    "    print(\"TRAIN INDEX:\", train_index)\n",
    "    print(\"TEST INDEX:\", test_index)\n",
    "    train_set = data[train_index]\n",
    "    test_set = data[test_index]\n",
    "    train_classes = classes[train_index]\n",
    "    test_classes = classes[test_index]\n",
    "# Note: this for loop is only executed once if n_splits==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(719, 96, 683, 1)\n",
      "(240, 96, 683, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Counts: Class 0: 216 Class 1: 503\n"
     ]
    }
   ],
   "source": [
    "print(\"Class Counts: Class 0:\", sum(train_classes==0), \"Class 1:\", sum(train_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating CNN Models in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compact CNN\n",
    "\n",
    "This is a 5 layer Convolutional Neural Network inspired and adapted from Keunwoo Choi (https://github.com/keunwoochoi/music-auto_tagging-keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(959, 96, 683, 1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CompactCNN(input_shape, nb_conv, nb_filters, n_mels, normalize, nb_hidden, dense_units, \n",
    "               output_shape, activation, dropout, multiple_segments=False, graph_model=False, input_tensor=None):\n",
    "    \n",
    "    melgram_input = Input(shape=input_shape)\n",
    "\n",
    "    if n_mels >= 256:\n",
    "        poolings = [(2, 4), (4, 4), (4, 5), (2, 4), (4, 4)]\n",
    "    elif n_mels >= 128:\n",
    "        poolings = [(2, 4), (4, 4), (2, 5), (2, 4), (4, 4)]\n",
    "    elif n_mels >= 96:\n",
    "        poolings = [(2, 4), (3, 4), (2, 5), (2, 4), (4, 4)]\n",
    "    elif n_mels >= 72:\n",
    "        poolings = [(2, 4), (3, 4), (2, 5), (2, 4), (3, 4)]\n",
    "    elif n_mels >= 64:\n",
    "        poolings = [(2, 4), (2, 4), (2, 5), (2, 4), (4, 4)]\n",
    "\n",
    "    # Determine input axis\n",
    "    if keras.backend.image_dim_ordering() == 'th':\n",
    "        channel_axis = 1\n",
    "        freq_axis = 2\n",
    "        time_axis = 3\n",
    "    else:\n",
    "        channel_axis = 3\n",
    "        freq_axis = 1\n",
    "        time_axis = 2\n",
    "            \n",
    "    # Input block\n",
    "    #x = BatchNormalization(axis=time_axis, name='bn_0_freq')(melgram_input)\n",
    "        \n",
    "    if normalize == 'batch':\n",
    "        x = BatchNormalization(axis=freq_axis, name='bn_0_freq')(melgram_input)\n",
    "    elif normalize in ('data_sample', 'time', 'freq', 'channel'):\n",
    "        x = Normalization2D(normalize, name='nomalization')(melgram_input)\n",
    "    elif normalize in ('no', 'False'):\n",
    "        x = melgram_input\n",
    "\n",
    "    # Conv block 1\n",
    "    x = Convolution2D(nb_filters[0], (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization(axis=channel_axis, name='bn1')(x)\n",
    "    x = ELU()(x)\n",
    "    x = MaxPooling2D(pool_size=poolings[0], name='pool1')(x)\n",
    "        \n",
    "    # Conv block 2\n",
    "    x = Convolution2D(nb_filters[1], (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization(axis=channel_axis, name='bn2')(x)\n",
    "    x = ELU()(x)\n",
    "    x = MaxPooling2D(pool_size=poolings[1], name='pool2')(x)\n",
    "        \n",
    "    # Conv block 3\n",
    "    x = Convolution2D(nb_filters[2], (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization(axis=channel_axis, name='bn3')(x)\n",
    "    x = ELU()(x)\n",
    "    x = MaxPooling2D(pool_size=poolings[2], name='pool3')(x)\n",
    "    \n",
    "    # Conv block 4\n",
    "    if nb_conv > 3:        \n",
    "        x = Convolution2D(nb_filters[3], (3, 3), padding='same')(x)\n",
    "        x = BatchNormalization(axis=channel_axis, name='bn4')(x)\n",
    "        x = ELU()(x)   \n",
    "        x = MaxPooling2D(pool_size=poolings[3], name='pool4')(x)\n",
    "        \n",
    "    # Conv block 5\n",
    "    if nb_conv == 5:\n",
    "        x = Convolution2D(nb_filters[4], (3, 3), padding='same')(x)\n",
    "        x = BatchNormalization(axis=channel_axis, name='bn5')(x)\n",
    "        x = ELU()(x)\n",
    "        x = MaxPooling2D(pool_size=poolings[4], name='pool5')(x)\n",
    "\n",
    "    # Flatten the outout of the last Conv Layer\n",
    "    x = Flatten()(x)\n",
    "      \n",
    "    if nb_hidden == 1:\n",
    "        x = Dropout(dropout)(x)\n",
    "        x = Dense(dense_units, activation='relu')(x)\n",
    "    elif nb_hidden == 2:\n",
    "        x = Dropout(dropout)(x)\n",
    "        x = Dense(dense_units[0], activation='relu')(x)\n",
    "        x = Dropout(dropout)(x)\n",
    "        x = Dense(dense_units[1], activation='relu')(x) \n",
    "    else:\n",
    "        raise ValueError(\"More than 2 hidden units not supported at the moment.\")\n",
    "    \n",
    "    # Output Layer\n",
    "    x = Dense(output_shape, activation=activation, name = 'output')(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(melgram_input, x)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set model parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of Convolutional Layers\n",
    "nb_conv_layers = 4\n",
    "\n",
    "# number of Filters in each layer\n",
    "nb_filters = [64,64,64,128,128]\n",
    "\n",
    "# number of hidden layers at the end of the model\n",
    "nb_hidden = 1 # 2\n",
    "\n",
    "# how many neurons in each hidden layer\n",
    "dense_units = 128 #[128,56]\n",
    "\n",
    "# how many output units\n",
    "# IN A BINARY CLASSIFICATION TASK with 2 possible outputs, 1 single output unit is sufficent (deciding between 0 and 1)\n",
    "output_shape = 1\n",
    "\n",
    "# which activation function to use for OUTPUT layer\n",
    "# IN A BINARY CLASSIFICATION TASK sigmoid activation is the right choice (activating betwee 0 and 1)\n",
    "output_activation = 'sigmoid'\n",
    "\n",
    "# which type of normalization\n",
    "normalization = 'batch'\n",
    "\n",
    "# droupout\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CompactCNN(input_shape, nb_conv = nb_conv_layers, nb_filters= nb_filters, n_mels = 96, \n",
    "                           normalize=normalization, \n",
    "                           nb_hidden = nb_hidden, dense_units = dense_units, \n",
    "                           output_shape = output_shape, activation = output_activation, \n",
    "                           dropout = dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 96, 683, 1)        0         \n",
      "_________________________________________________________________\n",
      "bn_0_freq (BatchNormalizatio (None, 96, 683, 1)        384       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 96, 683, 64)       640       \n",
      "_________________________________________________________________\n",
      "bn1 (BatchNormalization)     (None, 96, 683, 64)       256       \n",
      "_________________________________________________________________\n",
      "elu_1 (ELU)                  (None, 96, 683, 64)       0         \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling2D)         (None, 48, 170, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 48, 170, 64)       36928     \n",
      "_________________________________________________________________\n",
      "bn2 (BatchNormalization)     (None, 48, 170, 64)       256       \n",
      "_________________________________________________________________\n",
      "elu_2 (ELU)                  (None, 48, 170, 64)       0         \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling2D)         (None, 16, 42, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 42, 64)        36928     \n",
      "_________________________________________________________________\n",
      "bn3 (BatchNormalization)     (None, 16, 42, 64)        256       \n",
      "_________________________________________________________________\n",
      "elu_3 (ELU)                  (None, 16, 42, 64)        0         \n",
      "_________________________________________________________________\n",
      "pool3 (MaxPooling2D)         (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "bn4 (BatchNormalization)     (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "elu_4 (ELU)                  (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "pool4 (MaxPooling2D)         (None, 4, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 281,345\n",
      "Trainable params: 280,513\n",
      "Non-trainable params: 832\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "\n",
    "# the loss for a binary classification task is BINARY crossentropy\n",
    "loss = 'binary_crossentropy' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "\n",
    "# simple case:\n",
    "# Stochastic Gradient Descent\n",
    "#optimizer = 'sgd' \n",
    "\n",
    "# advanced:\n",
    "sgd = optimizers.SGD(momentum=0.9, nesterov=True)\n",
    "rmsprop = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.01)#lr=0.001 decay = 0.03\n",
    "adagrad = optimizers.Adagrad(lr=0.01, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "# We use mostly ADAM\n",
    "adam = optimizers.Adam(lr=0.003, beta_1=0.9, beta_2=0.999, epsilon=1e-07, decay=0.01)\n",
    "nadam = optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-07, schedule_decay=0.004)\n",
    "\n",
    "# choose\n",
    "optimizer = adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "metrics = ['accuracy', precision, recall]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other\n",
    "batch_size = 32 \n",
    "\n",
    "epochs = 30\n",
    "\n",
    "validation_split=0.1 \n",
    "\n",
    "#n_folds = 5\n",
    "random_seed = 0\n",
    "\n",
    "callbacks = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = os.getenv(\"HOME\")\n",
    "\n",
    "TB_LOGDIR = os.path.join(home_dir, \"tensorboard\")\n",
    "\n",
    "experiment_name = \"instrumental\"\n",
    "\n",
    "tb_logdir_cur = os.path.join(TB_LOGDIR, experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL\n",
    "# new tensorboard callback at each training\n",
    "# tensorboard_run_id = \"Vocal_magna_2seg_adam_compact_128fbis_128h\"\n",
    "# tb_logdir = \"%s/%s_fold%d %s\" %(tb_logdir, tensorboard_run_id, fold, strftime(\"%Y-%m-%d %H:%M:%S\", localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execute the following in a terminal:\n",
      "\n",
      "tensorboard --logdir=/home/tlidy/tensorboard\n"
     ]
    }
   ],
   "source": [
    "print(\"Execute the following in a terminal:\\n\")\n",
    "print(\"tensorboard --logdir=\" + TB_LOGDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize TensorBoard in Python\n",
    "tensorboard = TensorBoard(log_dir = tb_logdir_cur)\n",
    "\n",
    "# + add to callbacks\n",
    "callbacks = [tensorboard]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then open Tensorboard in browser:\n",
    "\n",
    "http://localhost:6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_crossentropy\n",
      "<keras.optimizers.Adam object at 0x7f34365d4898>\n",
      "['accuracy', <function precision at 0x7f34360c5840>, <function recall at 0x7f34360c5598>]\n",
      "Batch size: 32 Epochs: 30\n"
     ]
    }
   ],
   "source": [
    "# Summary of Training options\n",
    "\n",
    "print(loss)\n",
    "print(optimizer)\n",
    "print(metrics)\n",
    "print(\"Batch size:\", batch_size, \"Epochs:\", epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPILE MODEL\n",
    "\n",
    "model.compile(loss=loss, metrics=metrics, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# past_epochs is only for the case that we execute the next code box multiple times (so that Tensorboard is displaying properly)\n",
    "past_epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TRAINING\n",
    "\n",
    "history = model.fit(train_set, train_classes, \n",
    "                     validation_split=validation_split,\n",
    "                     #validation_data=(X_test,y_test), \n",
    "                     epochs=epochs, \n",
    "                     initial_epoch=past_epochs,\n",
    "                     batch_size=batch_size, \n",
    "                     callbacks=callbacks\n",
    "                     )\n",
    "\n",
    "past_epochs += epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Accuracy on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute probabilities for the classes (= get outputs of output layer)\n",
    "test_pred_prob = model.predict(test_set)\n",
    "test_pred_prob[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the predicted class we have to round 0 < 0.5 > 1\n",
    "test_pred = np.round(test_pred_prob)\n",
    "test_pred[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get final Accuracy\n",
    "accuracy_score(test_classes, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Genre Classification\n",
    "\n",
    "this is a single-label / multi-class task (multiple categories, but decision needs to be for 1 of them)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Metadata\n",
    "\n",
    "we start with the original metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which columns are available\n",
    "tags_all = metadata.columns.tolist()\n",
    "print(tags_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tags_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = ['classical', 'rock', 'pop', 'jazz', 'techno'] # 'electronic', ## too little data: , 'reggae', 'metal', 'hip hop']\n",
    "\n",
    "n_genres = len(genres)\n",
    "n_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[genres]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[genres].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[genres].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the single-label genre task, we only retain tracks that have EXACTLY 1 genre assigned in groundtruth\n",
    "idx = metadata[genres].sum(axis=1) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_metadata = metadata.loc[idx,genres]\n",
    "genre_metadata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_metadata.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes needs to be a \"1-hot encoded\" numpy array (which our groundtruth already is! we just convert pandas to numpy)\n",
    "classes = genre_metadata.values\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = genre_metadata.index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Audio Spectrograms\n",
    "\n",
    "based on the new filelist needed for the genre task \n",
    "\n",
    "we keep n_mel_bands and frames the same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we saved the audio spectrograms before, we try to load them\n",
    "load_features = True\n",
    "\n",
    "# if not, we store audio features for faster reload the next time\n",
    "save_features = True\n",
    "\n",
    "FEAT_FILE = os.path.join(DATA_PATH, \"spectrograms_genres.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_features:\n",
    "    if os.path.exists(FEAT_FILE):\n",
    "        with np.load(FEAT_FILE) as npz:\n",
    "            data = npz['data']\n",
    "            filelist = npz['filenames']\n",
    "            classes = npz['classes']\n",
    "        print(\"Loaded features successfully: \" + str(len(filelist)), \"files, dimensions:\", data.shape)\n",
    "    else:\n",
    "        load_features = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_features:\n",
    "    data = create_spectrograms(filelist, n_mel_bands, frames)\n",
    "\n",
    "    if save_features:\n",
    "        np.savez(FEAT_FILE, data=data, filenames=filelist, classes=classes)\n",
    "        print(\"Features stored to \" + FEAT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the data (see above)\n",
    "data = standardize(data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add color channel (see above)\n",
    "data = add_channel(data, n_channels=1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_shape: we store the new shape of the images in the 'input_shape' variable.\n",
    "# take all dimensions except the 0th one (which is the number of files)\n",
    "input_shape = data.shape[1:]  \n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Test Set Split\n",
    "\n",
    "We split the original full data set into two parts: Train Set (75%) and Test Set (25%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_size = 0.25 # % portion of whole data set to keep for testing, i.e. 75% is used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified Split retains the class balance in both sets\n",
    "\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=testset_size, random_state=0)\n",
    "splits = splitter.split(data, classes)\n",
    "\n",
    "for train_index, test_index in splits:\n",
    "    train_set = data[train_index]\n",
    "    test_set = data[test_index]\n",
    "    train_classes = classes[train_index]\n",
    "    test_classes = classes[test_index]\n",
    "# Note: this for loop is only executed once if n_splits==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Training Parameters\n",
    "\n",
    "we use the same model as for Instrumental vs. Vocal above\n",
    "\n",
    "with a few changes in the Training parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change #1: Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss for a single label classification task is CATEGORICAL crossentropy\n",
    "loss = 'categorical_crossentropy' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change #2: Output units and activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many output units\n",
    "# IN A SINGLE LABEL MULTI-CLASS TASK with N classes, we need N output units\n",
    "output_shape = n_genres\n",
    "\n",
    "# which activation function to use for OUTPUT layer\n",
    "# IN A SINGLE LABEL MULTI-CLASS TASK with N classes we use softmax activation to BALANCE best between the classes \n",
    "# and find the best decision for ONE class\n",
    "output_activation = 'softmax'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"genres\"\n",
    "\n",
    "tb_logdir_cur = os.path.join(TB_LOGDIR, experiment_name)\n",
    "\n",
    "# initialize TensorBoard in Python\n",
    "tensorboard = TensorBoard(log_dir = tb_logdir_cur)\n",
    "\n",
    "# + add to callbacks\n",
    "callbacks = [tensorboard]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rest of Parameters\n",
    "\n",
    "stay essentially the same (or similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = adam\n",
    "\n",
    "batch_size = 32 \n",
    "\n",
    "epochs = 30\n",
    "\n",
    "validation_split=0.1 \n",
    "\n",
    "random_seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of Training options\n",
    "\n",
    "print(loss)\n",
    "print(optimizer)\n",
    "print(metrics)\n",
    "print(\"Batch size:\", batch_size, \"Epochs:\", epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CompactCNN(input_shape, nb_conv = nb_conv_layers, nb_filters= nb_filters, n_mels = 96, \n",
    "                           normalize=normalization, \n",
    "                           nb_hidden = nb_hidden, dense_units = dense_units, \n",
    "                           output_shape = output_shape, activation = output_activation, \n",
    "                           dropout = dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPILE MODEL\n",
    "\n",
    "model.compile(loss=loss, metrics=metrics, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# past_epochs is only for the case that we execute the next code box multiple times (so that Tensorboard is displaying properly)\n",
    "past_epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# START TRAINING\n",
    "\n",
    "history = model.fit(train_set, train_classes, \n",
    "                     validation_split=validation_split,\n",
    "                     #validation_data=(X_test,y_test), \n",
    "                     epochs=epochs, \n",
    "                     initial_epoch=past_epochs,\n",
    "                     batch_size=batch_size, \n",
    "                     callbacks=callbacks\n",
    "                     )\n",
    "\n",
    "past_epochs += epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Accuracy on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute probabilities for the classes (= get outputs of output layer)\n",
    "test_pred_prob = model.predict(test_set)\n",
    "test_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the predicted class, we take the ARG MAX of the row vectors \n",
    "test_pred = np.argmax(test_pred_prob, axis=1)\n",
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same for groundtruth\n",
    "test_gt = np.argmax(test_classes, axis=1)\n",
    "test_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get final Accuracy\n",
    "accuracy_score(test_gt, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Mood Recognition\n",
    "\n",
    "this is a multi-label classification task (multiple categories to detect, any of them can be 0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = metadata.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', len(a))\n",
    "print(a)\n",
    "pd.reset_option('display.max_rows')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapt Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we select 5 moods from the original list of tags \n",
    "moods = ['funky', 'quiet', 'mellow','calm', 'sad'] ## too little data: 'happy','scary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and check the data on it\n",
    "#metadata[moods]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[moods].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the single-label genre task, we only retain tracks that have AT LEAST 1 of these moods assigned in groundtruth\n",
    "idx = metadata[moods].sum(axis=1) >= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mood_metadata = metadata.loc[idx,moods]\n",
    "mood_metadata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check\n",
    "mood_metadata.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mood_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes needs to be a MULTI-HOT encoded\" numpy array \n",
    "# (which our groundtruth already is! we just convert pandas to numpy)\n",
    "classes = mood_metadata.values\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = mood_metadata.index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Audio Spectrograms\n",
    "\n",
    "based on the new filelist needed for the mood task \n",
    "\n",
    "we keep n_mel_bands and frames the same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we saved the audio spectrograms before, we try to load them\n",
    "load_features = False\n",
    "\n",
    "# if not, we store audio features for faster reload the next time\n",
    "save_features = True\n",
    "\n",
    "FEAT_FILE = os.path.join(DATA_PATH, \"spectrograms_moods.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_features:\n",
    "    if os.path.exists(FEAT_FILE):\n",
    "        with np.load(FEAT_FILE) as npz:\n",
    "            data = npz['data']\n",
    "            filelist = npz['filenames']\n",
    "            classes = npz['classes']\n",
    "        print(\"Loaded features successfully: \" + str(len(filelist)), \"files, dimensions:\", data.shape)\n",
    "    else:\n",
    "        load_features = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_features:\n",
    "    data = create_spectrograms(filelist, n_mel_bands, frames)\n",
    "\n",
    "    if save_features:\n",
    "        np.savez(FEAT_FILE, data=data, filenames=filelist, classes=classes)\n",
    "        print(\"Features stored to \" + FEAT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the data (see above)\n",
    "data = standardize(data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add color channel (see above)\n",
    "data = add_channel(data, n_channels=1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_shape: we store the new shape of the images in the 'input_shape' variable.\n",
    "# take all dimensions except the 0th one (which is the number of files)\n",
    "input_shape = data.shape[1:]  \n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Test Set Split\n",
    "\n",
    "We split the original full data set into two parts: Train Set (75%) and Test Set (25%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change: We cannot use Stratified Split here as it does not make sense for a MULTI-LABEL TASK!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use ShuffleSplit INSTEAD OF StratifiedShuffleSplit \n",
    "\n",
    "splitter = ShuffleSplit(n_splits=1, test_size=testset_size, random_state=0)\n",
    "splits = splitter.split(data, classes)\n",
    "\n",
    "for train_index, test_index in splits:\n",
    "    train_set = data[train_index]\n",
    "    test_set = data[test_index]\n",
    "    train_classes = classes[train_index]\n",
    "    test_classes = classes[test_index]\n",
    "# Note: this for loop is only executed once if n_splits==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Training Parameters\n",
    "\n",
    "we use the same model as for Instrumental vs. Vocal and Genres above\n",
    "\n",
    "with a few changes in the Training parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change #1: Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss for a MULTI label classification task is BINARY crossentropy\n",
    "loss = 'binary_crossentropy' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change #2: Output units and activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many output units\n",
    "# IN A SINGLE-LABEL MULTI-CLASS or MULTI-LABEL TASK with N classes, we need N output units\n",
    "\n",
    "output_shape = n_genres\n",
    "\n",
    "# which activation function to use for OUTPUT layer\n",
    "# IN A MULTI-LABEL TASK with N classes we use SIGMOID activation same as with a BINARY task\n",
    "# as EACH of the classes can be 0 or 1 \n",
    "\n",
    "output_activation = 'sigmoid'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"moods\"\n",
    "\n",
    "tb_logdir_cur = os.path.join(TB_LOGDIR, experiment_name)\n",
    "\n",
    "# initialize TensorBoard in Python\n",
    "tensorboard = TensorBoard(log_dir = tb_logdir_cur)\n",
    "\n",
    "# + add to callbacks\n",
    "callbacks = [tensorboard]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rest of Parameters\n",
    "\n",
    "stay essentially the same (or similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = adam\n",
    "\n",
    "batch_size = 32 \n",
    "\n",
    "epochs = 30\n",
    "\n",
    "validation_split=0.1 \n",
    "\n",
    "random_seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of Training options\n",
    "\n",
    "print(loss)\n",
    "print(optimizer)\n",
    "print(metrics)\n",
    "print(\"Batch size:\", batch_size, \"Epochs:\", epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPILE MODEL\n",
    "\n",
    "model.compile(loss=loss, metrics=metrics, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# past_epochs is only for the case that we execute the next code box multiple times (so that Tensorboard is displaying properly)\n",
    "past_epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TRAINING\n",
    "\n",
    "history = model.fit(train_set, train_classes, \n",
    "                     validation_split=validation_split,\n",
    "                     #validation_data=(X_test,y_test), \n",
    "                     epochs=epochs, \n",
    "                     initial_epoch=past_epochs,\n",
    "                     batch_size=batch_size, \n",
    "                     callbacks=callbacks\n",
    "                     )\n",
    "\n",
    "past_epochs += epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Accuracy on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute probabilities for the classes (= get outputs of output layer)\n",
    "test_pred_prob = model.predict(test_set)\n",
    "test_pred_prob[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the predicted class we have to round 0 < 0.5 > 1\n",
    "test_pred = np.round(test_pred_prob)\n",
    "test_pred[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get final Accuracy\n",
    "accuracy_score(test_classes, test_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
